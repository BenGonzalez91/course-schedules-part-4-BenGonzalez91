{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Step by Step from the Very Beginning\n",
    "Follow the instructions in the Markdown cells to fill in the code cells below. We are going to build up a function to scrape an HTML file using Beautiful Soup, a few lines at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules, installing any missing libraries\n",
    "This one already done for you.\n",
    "Beautiful Soup should already be installed. If not then type \n",
    "`pip install beautifulsoup4` into a terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open the HTML file for scraping\n",
    "Use a `with` statement to open the `Spring2018ClassSchedules.html` file and print out the first line to the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    print(fp.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape out the HTML table rows\n",
    "Use beautiful soup to select and print out the first 5 table rows, one row at a time. Take a moment to guess the structure of the HTML from the print out. What is the difference between dddheader and dddefault? WHat about <tr>, <td>, and <th>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    # create an HTML parse tree for the document; we can then select specific subtrees\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    # select all subtrees that represent a table row\n",
    "    # the find and find_all methods are defined in the Beautiful Soup docs\n",
    "    data_display_table_rows = soup.find('table',class_='datadisplaytable').find_all('tr')\n",
    "    \n",
    "    # print the first 5 table rows\n",
    "    print(data_display_table_rows[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scrape out the course CRNs \n",
    "Use a selector to just print the CRN column in the first 5 rows. Based on the HTML above, it appears that the CRN is in the second column, just after the open/closed status.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    # create an HTML parse tree for the document; we can then select specific subtrees\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    # select all subtrees that represent a table row\n",
    "    # the find and find_all methods are defined in the Beautiful Soup docs\n",
    "    data_display_table_rows = soup.find('table',class_='datadisplaytable').find_all('tr')\n",
    "    \n",
    "    for row in data_display_table_rows[:5]:\n",
    "        \n",
    "        # select the data columns; result is [] if no data columns\n",
    "        # the .dddefault css class is used in the HTML to indicate normal (non-header) table cells\n",
    "        cols = row.select(\"td.dddefault\")\n",
    "        \n",
    "        if (cols):\n",
    "            # the string attribute represents the visible text content (i.e., not HTML markup)\n",
    "            # crn is in col 1 in the cols list\n",
    "            # if the crn is missing then we get '\\xa0' instead of a blank string\n",
    "            print(cols[1].string.strip('\\xa0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scrape out the other columns for any row that has a CRN\n",
    "This is mostly formulaic, just like CRN but for the other columns. The scraped data record is then returned as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    # create an HTML parse tree for the document; we can then select specific subtrees\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    # select all subtrees that represent a table row\n",
    "    # the find and find_all methods are defined in the Beautiful Soup docs\n",
    "    data_display_table_rows = soup.find('table',class_='datadisplaytable').find_all('tr')\n",
    "    \n",
    "    for row in data_display_table_rows[:5]:\n",
    "    \n",
    "        # select the data columns; result is [] if no data columns\n",
    "        # the .dddefault css class is used in the HTML to indicate normal (non-header) table cells\n",
    "        cols = row.select(\"td.dddefault\")\n",
    "        \n",
    "        # initialize a blank dictionary for the course data\n",
    "        course_spec={}\n",
    "        if (cols and cols[1].string.strip('\\xa0')):\n",
    "            course_spec['crn'] = cols[1].string\n",
    "            course_spec['catalogid'] = cols[2].string + \" \" + cols[3].string\n",
    "            course_spec['timecodes'] = [cols[8].string+\" \"+cols[9].string+\" \"+cols[17].string]\n",
    "            course_spec['section'] = cols[4].string\n",
    "            course_spec['credits'] = cols[6].string\n",
    "            course_spec['title'] = cols[7].string\n",
    "            course_spec['instructor'] = cols[16].get_text()[:-4]\n",
    "            \n",
    "            print(course_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deal with extra timecodes.\n",
    "This requires us to look at multiple lines, with the extra timecodes listed with a blank CRN. (See AC 204 for a few examples.) So, we'll accumulate a list of all course_specs instead of printing them one line at a time. That lets us add the extra timecodes to existing courses as we find them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    # create an HTML parse tree for the document; we can then select specific subtrees\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    # select all subtrees that represent a table row\n",
    "    # the find and find_all methods are defined in the Beautiful Soup docs\n",
    "    data_display_table_rows = soup.find('table',class_='datadisplaytable').find_all('tr')\n",
    "    \n",
    "    # The list of courses scraped from the file\n",
    "    course_specs = []\n",
    "    \n",
    "    # Each row is for a single course, but there may be extra lines for multiple timecodes\n",
    "    for row in data_display_table_rows[:30]:\n",
    "    \n",
    "        # select the data columns; result is [] if no data columns\n",
    "        # the .dddefault css class is used in the HTML to indicate normal (non-header) table cells\n",
    "        cols = row.select(\"td.dddefault\")\n",
    "        \n",
    "        # cols is empty for table header rows\n",
    "        if cols:\n",
    "            \n",
    "            # cols[1] is empty for \"extra timecode\" rows\n",
    "            crn = cols[1].string.strip('\\xa0')\n",
    "            if crn:\n",
    "                # the normal case\n",
    "                \n",
    "                # pick off the columns and stuff into a dict\n",
    "                course= {\n",
    "                    'crn' : crn,\n",
    "                    'catalogid' : cols[2].string + \" \" + cols[3].string,\n",
    "                    'timecodes' : [cols[8].string+\" \"+cols[9].string+\" \"+cols[17].string],\n",
    "                    'section' : cols[4].string,\n",
    "                    'credits' : cols[6].string,\n",
    "                    'title' : cols[7].string,\n",
    "                    'instructor' : cols[16].get_text()[:-4]\n",
    "                }\n",
    "                \n",
    "                # add the dict to the course_specs list\n",
    "                course_specs += [course]\n",
    "                \n",
    "            else:\n",
    "                # the extra timecodes case\n",
    "                print(len(course_specs))\n",
    "                course_specs[-1]['timecodes'] += [cols[8].string+\" \"+cols[9].string+\" \"+cols[17].string]\n",
    "\n",
    "course_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean up potential data anomalies\n",
    "There are two issues here\n",
    "- CRNs are integers, not strings\n",
    "- In columns without data Beautiful Soup will return a nonbreaking space (character '\\xa0'). We stripped this out for the CRN above but it can also happen in other columns. We should strip it out in all the other columns too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Spring2018ClassSchedules.html\") as fp:\n",
    "    # create an HTML parse tree for the document; we can then select specific subtrees\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    # select all subtrees that represent a table row\n",
    "    # the find and find_all methods are defined in the Beautiful Soup docs\n",
    "    data_display_table_rows = soup.find('table',class_='datadisplaytable').find_all('tr')\n",
    "    \n",
    "    # The list of courses scraped from the file\n",
    "    course_specs = []\n",
    "    \n",
    "    # Each row is for a single course, but there may be extra lines for multiple timecodes\n",
    "    for row in data_display_table_rows[:20]:\n",
    "    \n",
    "        # select the data columns; result is [] if no data columns\n",
    "        # the .dddefault css class is used in the HTML to indicate normal (non-header) table cells\n",
    "        cols = row.select(\"td.dddefault\")\n",
    "        \n",
    "        # cols is empty for table header rows\n",
    "        if cols:\n",
    "            \n",
    "            # cols[1] is empty for \"extra timecode\" rows\n",
    "            crn = cols[1].string.strip('\\xa0')\n",
    "            if crn:\n",
    "                # the normal case\n",
    "                \n",
    "                # pick off the columns and stuff into a dict\n",
    "                course= {\n",
    "                    'crn' : int(crn),\n",
    "                    'catalogid' : (cols[2].string + \" \" + cols[3].string).strip('\\xa0'),\n",
    "                    'timecodes' : [(cols[8].string+\" \"+cols[9].string+\" \"+cols[17].string).strip('\\xa0')],\n",
    "                    'section' : cols[4].string.strip('\\xa0'),\n",
    "                    'credits' : cols[6].string.strip('\\xa0'),\n",
    "                    'title' : cols[7].string.strip('\\xa0'),\n",
    "                    'instructor' : (cols[16].get_text()[:-4]).strip('\\xa0')\n",
    "                }\n",
    "                \n",
    "                # add the dict to the course_specs list\n",
    "                course_specs += [course]\n",
    "                \n",
    "            else:\n",
    "                # the extra timecodes case\n",
    "                print(len(course_specs))\n",
    "                course_specs[-1]['timecodes'] += [(cols[8].string+\" \"+cols[9].string+\" \"+cols[17].string).strip('\\xa0')]\n",
    "\n",
    "course_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The finished code\n",
    "The `course_schedules_beautiful_soup.py` module (in this folder) has a slightly more refined version of ths code that ...\n",
    "- wraps the scraping code above into a function called  `scrape_banner_course_schedule()` \n",
    "- provides a `banner_col` dict that maps column names to column numbers (just in case Banner changes column order)\n",
    "- defines a `json_dump()` utility function that we can use to be 100% sure that the data in JSON-formatted if needed\n",
    "- cleans up extraneous spaces in the timecodes\n",
    "- includes (commented out) test code so that the file can be run from the command line or a debugger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import course_schedules_beautiful_soup as csbs\n",
    "course_offerings = csbs.scrape_banner_course_schedule('Spring2018ClassSchedules.html')\n",
    "csbs.json_dump(course_offerings,\"Spring2018ClassSchedules.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
